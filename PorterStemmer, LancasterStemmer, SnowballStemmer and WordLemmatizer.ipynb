{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b419217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, RegexpStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text documents\n",
    "documents = [\n",
    "    \"The cats are running and jumping in the garden.\",\n",
    "    \"She is a beautiful runner and loves to run fast.\",\n",
    "    \"Running helps to build stamina and strength.\",\n",
    "    \"He ran swiftly and caught the ball.\"\n",
    "]\n",
    "\n",
    "# Initialize stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Define a simple regex for stemming (this can be customized)\n",
    "regex_pattern = r'(ing|ed|es|s)$'\n",
    "regex_stemmer = RegexpStemmer(regex_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6a6b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Document: The cats are running and jumping in the garden.\n",
      "Porter Stems: ['the', 'cat', 'are', 'run', 'and', 'jump', 'in', 'the', 'garden', '.']\n",
      "Lancaster Stems: ['the', 'cat', 'ar', 'run', 'and', 'jump', 'in', 'the', 'gard', '.']\n",
      "Snowball Stems: ['the', 'cat', 'are', 'run', 'and', 'jump', 'in', 'the', 'garden', '.']\n",
      "Regex Stems: ['the', 'cat', 'are', 'runn', 'and', 'jump', 'in', 'the', 'garden', '.']\n",
      "\n",
      "Original Document: She is a beautiful runner and loves to run fast.\n",
      "Porter Stems: ['she', 'is', 'a', 'beauti', 'runner', 'and', 'love', 'to', 'run', 'fast', '.']\n",
      "Lancaster Stems: ['she', 'is', 'a', 'beauty', 'run', 'and', 'lov', 'to', 'run', 'fast', '.']\n",
      "Snowball Stems: ['she', 'is', 'a', 'beauti', 'runner', 'and', 'love', 'to', 'run', 'fast', '.']\n",
      "Regex Stems: ['she', 'i', 'a', 'beautiful', 'runner', 'and', 'lov', 'to', 'run', 'fast', '.']\n",
      "\n",
      "Original Document: Running helps to build stamina and strength.\n",
      "Porter Stems: ['run', 'help', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "Lancaster Stems: ['run', 'help', 'to', 'build', 'stamin', 'and', 'strength', '.']\n",
      "Snowball Stems: ['run', 'help', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "Regex Stems: ['runn', 'help', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "\n",
      "Original Document: He ran swiftly and caught the ball.\n",
      "Porter Stems: ['he', 'ran', 'swiftli', 'and', 'caught', 'the', 'ball', '.']\n",
      "Lancaster Stems: ['he', 'ran', 'swift', 'and', 'caught', 'the', 'bal', '.']\n",
      "Snowball Stems: ['he', 'ran', 'swift', 'and', 'caught', 'the', 'ball', '.']\n",
      "Regex Stems: ['he', 'ran', 'swiftly', 'and', 'caught', 'the', 'ball', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to apply different stemmers\n",
    "def apply_stemmers(documents):\n",
    "    results = {}\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Tokenize the document\n",
    "        tokens = word_tokenize(doc.lower())\n",
    "        \n",
    "        # Apply different stemmers\n",
    "        porter_stems = [porter_stemmer.stem(token) for token in tokens]\n",
    "        lancaster_stems = [lancaster_stemmer.stem(token) for token in tokens]\n",
    "        snowball_stems = [snowball_stemmer.stem(token) for token in tokens]\n",
    "        regex_stems = [regex_stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # Store results\n",
    "        results[doc] = {\n",
    "            'porter': porter_stems,\n",
    "            'lancaster': lancaster_stems,\n",
    "            'snowball': snowball_stems,\n",
    "            'regex': regex_stems\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Apply the stemmers to the sample documents\n",
    "stemmed_results = apply_stemmers(documents)\n",
    "\n",
    "# Print the results\n",
    "for original_doc, stems in stemmed_results.items():\n",
    "    print(f\"\\nOriginal Document: {original_doc}\")\n",
    "    for stemmer_name, stemmed_words in stems.items():\n",
    "        print(f\"{stemmer_name.capitalize()} Stems: {stemmed_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4528a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: ['The', 'cats', 'are', 'running', 'and', 'jumping', 'in', 'the', 'garden', '.']\n",
      "Lemmatized: ['The', 'cat', 'be', 'run', 'and', 'jump', 'in', 'the', 'garden', '.']\n",
      "----------------------------------------\n",
      "Original Document: ['She', 'is', 'a', 'beautiful', 'runner', 'and', 'loves', 'to', 'run', 'fast', '.']\n",
      "Lemmatized: ['She', 'be', 'a', 'beautiful', 'runner', 'and', 'love', 'to', 'run', 'fast', '.']\n",
      "----------------------------------------\n",
      "Original Document: ['Running', 'helps', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "Lemmatized: ['Running', 'help', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "----------------------------------------\n",
      "Original Document: ['He', 'ran', 'swiftly', 'and', 'caught', 'the', 'ball', '.']\n",
      "Lemmatized: ['He', 'run', 'swiftly', 'and', 'catch', 'the', 'ball', '.']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sample text documents\n",
    "documents = [\n",
    "    \"The cats are running and jumping in the garden.\",\n",
    "    \"She is a beautiful runner and loves to run fast.\",\n",
    "    \"Running helps to build stamina and strength.\",\n",
    "    \"He ran swiftly and caught the ball.\"\n",
    "]\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply lemmatization\n",
    "def apply_lemmatization(doc):\n",
    "    # Tokenize the document\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # Apply lemmatization to each token\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token,pos='v') for token in tokens]\n",
    "    \n",
    "    return {\n",
    "        \"original\": tokens,\n",
    "        \"lemmatized\": lemmatized_tokens\n",
    "    }\n",
    "\n",
    "# Process each document and print results\n",
    "for doc in documents:\n",
    "    results = apply_lemmatization(doc)\n",
    "    print(f\"Original Document: {results['original']}\")\n",
    "    print(f\"Lemmatized: {results['lemmatized']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ce08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
